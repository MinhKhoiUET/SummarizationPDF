{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8908421,"sourceType":"datasetVersion","datasetId":5188823},{"sourceId":8908460,"sourceType":"datasetVersion","datasetId":5356400}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-18T15:44:57.248849Z","iopub.execute_input":"2024-07-18T15:44:57.249195Z","iopub.status.idle":"2024-07-18T15:44:58.507794Z","shell.execute_reply.started":"2024-07-18T15:44:57.249150Z","shell.execute_reply":"2024-07-18T15:44:58.506907Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/pdf-data/Meetbrain2Music.pdf\n/kaggle/input/data4summarization/Meetbrain2Music.pdf\n/kaggle/input/data4summarization/Hallucination_in_Large_Language_Models.txt\n/kaggle/input/data4summarization/Meetbrain2Mucsic.txt\n/kaggle/input/data4summarization/artificial_intelligence_wikipedia.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install langchain huggingface_hub tiktoken\n!pip install together","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:44:58.509447Z","iopub.execute_input":"2024-07-18T15:44:58.509855Z","iopub.status.idle":"2024-07-18T15:45:39.892651Z","shell.execute_reply.started":"2024-07-18T15:44:58.509827Z","shell.execute_reply":"2024-07-18T15:45:39.891712Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.2.9-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.2)\nCollecting tiktoken\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.20 (from langchain)\n  Downloading langchain_core-0.2.21-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.90-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.20->langchain) (1.33)\nCollecting packaging>=20.9 (from huggingface_hub)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m855.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.20->langchain) (2.4)\nDownloading langchain-0.2.9-py3-none-any.whl (987 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.7/987.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.21-py3-none-any.whl (372 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.0/372.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.90-py3-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.7/134.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, tiktoken, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.2.9 langchain-core-0.2.21 langchain-text-splitters-0.2.2 langsmith-0.1.90 orjson-3.10.6 packaging-24.1 tiktoken-0.7.0\nCollecting together\n  Downloading together-1.2.1-py3-none-any.whl.metadata (11 kB)\nCollecting aiohttp<4.0.0,>=3.9.3 (from together)\n  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.7 in /opt/conda/lib/python3.10/site-packages (from together) (8.1.7)\nCollecting eval-type-backport<0.3.0,>=0.1.3 (from together)\n  Downloading eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: filelock<4.0.0,>=3.13.1 in /opt/conda/lib/python3.10/site-packages (from together) (3.13.1)\nRequirement already satisfied: numpy>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from together) (1.26.4)\nCollecting pillow<11.0.0,>=10.3.0 (from together)\n  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\nRequirement already satisfied: pyarrow>=10.0.1 in /opt/conda/lib/python3.10/site-packages (from together) (14.0.2)\nCollecting pydantic<3.0.0,>=2.6.3 (from together)\n  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/conda/lib/python3.10/site-packages (from together) (2.32.3)\nRequirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from together) (0.9.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.66.2 in /opt/conda/lib/python3.10/site-packages (from together) (4.66.4)\nRequirement already satisfied: typer<0.13,>=0.9 in /opt/conda/lib/python3.10/site-packages (from together) (0.9.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.9.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.3->together) (4.0.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.6.3->together) (0.6.0)\nCollecting pydantic-core==2.20.1 (from pydantic<3.0.0,>=2.6.3->together)\n  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.6.3->together) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->together) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->together) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->together) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->together) (2024.2.2)\nDownloading together-1.2.1-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\nDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic-core, pillow, eval-type-backport, pydantic, aiohttp, together\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.14.6\n    Uninstalling pydantic_core-2.14.6:\n      Successfully uninstalled pydantic_core-2.14.6\n  Attempting uninstall: pillow\n    Found existing installation: Pillow 9.5.0\n    Uninstalling Pillow-9.5.0:\n      Successfully uninstalled Pillow-9.5.0\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.3\n    Uninstalling pydantic-2.5.3:\n      Successfully uninstalled pydantic-2.5.3\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.9.1\n    Uninstalling aiohttp-3.9.1:\n      Successfully uninstalled aiohttp-3.9.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiohttp-3.9.5 eval-type-backport-0.2.0 pillow-10.3.0 pydantic-2.7.2 pydantic-core-2.18.3 together-1.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pymupdf","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:45:39.894089Z","iopub.execute_input":"2024-07-18T15:45:39.894425Z","iopub.status.idle":"2024-07-18T15:45:55.827883Z","shell.execute_reply.started":"2024-07-18T15:45:39.894395Z","shell.execute_reply":"2024-07-18T15:45:55.826862Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting pymupdf\n  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting PyMuPDFb==1.24.6 (from pymupdf)\n  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\nDownloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\nSuccessfully installed PyMuPDFb-1.24.6 pymupdf-1.24.7\n","output_type":"stream"}]},{"cell_type":"code","source":"# import os\n\nos.environ[\"TOGETHER_API_KEY\"] = \"483eb57ad1aa199b962be798d9116cba588373f8efb8acf918e6b26eeeba42ac\"","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:47:59.649689Z","iopub.execute_input":"2024-07-18T15:47:59.649967Z","iopub.status.idle":"2024-07-18T15:47:59.654575Z","shell.execute_reply.started":"2024-07-18T15:47:59.649944Z","shell.execute_reply":"2024-07-18T15:47:59.653419Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"text_to_summarize = \"\"\"\nNew York City comprises 5 boroughs sitting where the Hudson River meets the Atlantic Ocean. At its core is Manhattan, a densely populated borough that’s among the world’s major commercial, financial and cultural centers. Its iconic sites include skyscrapers such as the Empire State Building and sprawling Central Park. Broadway theater is staged in neon-lit Times Square.\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:45:55.837745Z","iopub.execute_input":"2024-07-18T15:45:55.838012Z","iopub.status.idle":"2024-07-18T15:45:55.849140Z","shell.execute_reply.started":"2024-07-18T15:45:55.837988Z","shell.execute_reply":"2024-07-18T15:45:55.848161Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/data4summarization/Meetbrain2Mucsic.txt', 'r', encoding='utf-8') as f:\n    conference = f.read()","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:45:55.850449Z","iopub.execute_input":"2024-07-18T15:45:55.851222Z","iopub.status.idle":"2024-07-18T15:45:55.867083Z","shell.execute_reply.started":"2024-07-18T15:45:55.851168Z","shell.execute_reply":"2024-07-18T15:45:55.866328Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import fitz \npdf_path=r\"/kaggle/input/pdf-data/Meetbrain2Music.pdf\"\ndoc = fitz.open(pdf_path)\nprint(len(doc))","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:45:55.868515Z","iopub.execute_input":"2024-07-18T15:45:55.868815Z","iopub.status.idle":"2024-07-18T15:45:55.964209Z","shell.execute_reply.started":"2024-07-18T15:45:55.868790Z","shell.execute_reply":"2024-07-18T15:45:55.963209Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}]},{"cell_type":"code","source":"def extract_section_from_pages(doc, page_start, page_end):\n    content = \"\"\n    for page_num in range(page_start, page_end):\n        page = doc.load_page(page_num)\n        page_text = page.get_text(\"blocks\")\n        for block in page_text:\n            x0, y0, x1, y1, text = block[:5]\n            if not (x1 < 100 and y0 < 100) and not (y0 > page.rect.height - 50):\n                content += text + \" \"\n    return content;","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:45:55.965661Z","iopub.execute_input":"2024-07-18T15:45:55.966063Z","iopub.status.idle":"2024-07-18T15:45:55.973616Z","shell.execute_reply.started":"2024-07-18T15:45:55.966026Z","shell.execute_reply":"2024-07-18T15:45:55.972469Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"reference = extract_section_from_pages(doc, 0, len(doc) - 1);\nprint(len(reference))","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:53:11.041654Z","iopub.execute_input":"2024-07-18T15:53:11.042355Z","iopub.status.idle":"2024-07-18T15:53:11.077100Z","shell.execute_reply.started":"2024-07-18T15:53:11.042319Z","shell.execute_reply":"2024-07-18T15:53:11.076086Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"33601\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom together import Together\n\nclient = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n# meta-llama/Llama-3-70b-chat-hf\n# meta-llama/Llama-3-8b-chat-hf\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-3-70b-chat-hf\",\n    messages=[\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text. Please provide a detailed summary that is approximately one-half the length of the original text\"},\n      {\"role\": \"user\", \"content\": f\"Please summarize the following text: {reference}\"},\n    ],\n)\n\n\nprint(response.choices[0].message.content)\ncandidate = response.choices[0].message.content","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:53:12.186430Z","iopub.execute_input":"2024-07-18T15:53:12.187121Z","iopub.status.idle":"2024-07-18T15:53:19.817233Z","shell.execute_reply.started":"2024-07-18T15:53:12.187074Z","shell.execute_reply":"2024-07-18T15:53:19.816218Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Here is a detailed summary of the text:\n\n**Introduction**\n\nThe paper explores the possibility of reconstructing music from brain activity using a music generation model called MusicLM. The authors investigate whether it is possible to predict music embeddings from fMRI scans and use them to generate music that semantically resembles the original music stimulus.\n\n**Methods**\n\nThe authors use a music dataset with 540 music pieces from 10 genres, each with a 15-second clip. They also use a text caption dataset with descriptions of the music pieces. The MusicLM model is used to generate music conditioned on a MuLan embedding, which is a joint text/music embedding model. The authors predict the MuLan embedding from fMRI scans using a linear regression model and then use the predicted embedding to generate music.\n\n**Decoding Results**\n\nThe authors report the identification accuracy of the predicted music embeddings and find that MuLanmusic embeddings can be more accurately predicted from fMRI signals than other embedding types. They also find that the reconstructed music is semantically similar to the original stimulus, but the temporal structure is not preserved. The authors compare the retrieval and generation methods and find that both methods produce similar results.\n\n**Encoding Results**\n\nThe authors investigate the correspondence between the internal representations of MusicLM and recorded brain activity. They find that MuLan embeddings tend to have higher prediction performance in the lateral prefrontal cortex than w2v-BERT-avg embeddings, suggesting that MuLan captures high-level music information processed in the human brain. They also find that both embeddings have some degree of correspondence with human brain activity in the auditory cortex.\n\n**Generalization Beyond Music Genre**\n\nThe authors investigate whether their model can generalize to music genres that were not used during training. They ablate one genre during training and determine the identification accuracy on the held-out genre. They find that the model can generalize to some extent, but the performance is lower than when the genre is included in the training set.\n\nOverall, the paper demonstrates the possibility of reconstructing music from brain activity using a music generation model and provides insights into the neural representation of music in the human brain.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install nltk","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:45:58.350347Z","iopub.execute_input":"2024-07-18T15:45:58.350778Z","iopub.status.idle":"2024-07-18T15:46:11.768413Z","shell.execute_reply.started":"2024-07-18T15:45:58.350748Z","shell.execute_reply":"2024-07-18T15:46:11.767469Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:46:11.770099Z","iopub.execute_input":"2024-07-18T15:46:11.771045Z","iopub.status.idle":"2024-07-18T15:46:13.071132Z","shell.execute_reply.started":"2024-07-18T15:46:11.771012Z","shell.execute_reply":"2024-07-18T15:46:13.070107Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"score = sentence_bleu(conference.split(), candidate.split())\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:49:19.343273Z","iopub.execute_input":"2024-07-18T15:49:19.343633Z","iopub.status.idle":"2024-07-18T15:49:19.704807Z","shell.execute_reply.started":"2024-07-18T15:49:19.343605Z","shell.execute_reply":"2024-07-18T15:49:19.703768Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"0.27673806768630216\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow tensorflow-hub","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:46:13.375729Z","iopub.execute_input":"2024-07-18T15:46:13.376025Z","iopub.status.idle":"2024-07-18T15:46:30.246566Z","shell.execute_reply.started":"2024-07-18T15:46:13.376000Z","shell.execute_reply":"2024-07-18T15:46:30.245049Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: tensorflow-hub in /opt/conda/lib/python3.10/site-packages (0.16.1)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.59.3)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub) (2.15.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.3.3\n    Uninstalling keras-3.3.3:\n      Successfully uninstalled keras-3.3.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\n\n# Load Universal Sentence Encoder model\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\nmodel = hub.load(module_url)\n\nembeddings = model([conference, candidate])\n\nsimilarity_score = tf.tensordot(embeddings[0], embeddings[1], axes=1).numpy()\n\nprint(f\"Similarity score between summary_MarkTechPost and summary: {similarity_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:46:30.248336Z","iopub.execute_input":"2024-07-18T15:46:30.248987Z","iopub.status.idle":"2024-07-18T15:47:06.568236Z","shell.execute_reply.started":"2024-07-18T15:46:30.248928Z","shell.execute_reply":"2024-07-18T15:47:06.567230Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2024-07-18 15:46:32.500974: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-18 15:46:32.501084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-18 15:46:32.683877: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Similarity score between summary_MarkTechPost and summary: 0.7701590061187744\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculateSemanticSimilarity(conference, candidate):\n    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n    model = hub.load(module_url)\n\n    embeddings = model([conference, candidate])\n\n    similarity_score = tf.tensordot(embeddings[0], embeddings[1], axes=1).numpy()\n    return similarity_score","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:47:06.569594Z","iopub.execute_input":"2024-07-18T15:47:06.569902Z","iopub.status.idle":"2024-07-18T15:47:06.575153Z","shell.execute_reply.started":"2024-07-18T15:47:06.569875Z","shell.execute_reply":"2024-07-18T15:47:06.574204Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print (calculateSemanticSimilarity(conference, candidate))","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:49:25.248968Z","iopub.execute_input":"2024-07-18T15:49:25.249360Z","iopub.status.idle":"2024-07-18T15:49:30.804737Z","shell.execute_reply.started":"2024-07-18T15:49:25.249331Z","shell.execute_reply":"2024-07-18T15:49:30.803762Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"0.83325404\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install openai tiktoken tqdm","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:47:16.321507Z","iopub.execute_input":"2024-07-18T15:47:16.321794Z","iopub.status.idle":"2024-07-18T15:47:30.815111Z","shell.execute_reply.started":"2024-07-18T15:47:16.321771Z","shell.execute_reply":"2024-07-18T15:47:30.814052Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting openai\n  Downloading openai-1.35.14-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.7.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.7.2)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\nRequirement already satisfied: pydantic-core==2.18.3 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.18.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\nDownloading openai-1.35.14-py3-none-any.whl (328 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: openai\nSuccessfully installed openai-1.35.14\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom typing import List, Tuple, Optional\nfrom openai import OpenAI\nimport tiktoken\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:47:30.816654Z","iopub.execute_input":"2024-07-18T15:47:30.816944Z","iopub.status.idle":"2024-07-18T15:47:31.259023Z","shell.execute_reply.started":"2024-07-18T15:47:30.816918Z","shell.execute_reply":"2024-07-18T15:47:31.258193Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def get_chat_completion(messages, model='meta-llama/Llama-3-8b-chat-hf'):\n    response1 = client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    return response1.choices[0].message.content","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:47:31.260364Z","iopub.execute_input":"2024-07-18T15:47:31.260660Z","iopub.status.idle":"2024-07-18T15:47:31.265572Z","shell.execute_reply.started":"2024-07-18T15:47:31.260635Z","shell.execute_reply":"2024-07-18T15:47:31.264753Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def tokenize(text: str) -> List[str]:\n    encoding = tiktoken.encoding_for_model('gpt-4-turbo')\n    return encoding.encode(text)\n\n\n# This function chunks a text into smaller pieces based on a maximum token count and a delimiter.\ndef chunk_on_delimiter(input_string: str,\n                       max_tokens: int, delimiter: str) -> List[str]:\n    chunks = input_string.split(delimiter)\n    combined_chunks, _, dropped_chunk_count = combine_chunks_with_no_minimum(\n        chunks, max_tokens, chunk_delimiter=delimiter, add_ellipsis_for_overflow=True\n    )\n    if dropped_chunk_count > 0:\n        print(f\"warning: {dropped_chunk_count} chunks were dropped due to overflow\")\n    combined_chunks = [f\"{chunk}{delimiter}\" for chunk in combined_chunks]\n    return combined_chunks\n\n\n# This function combines text chunks into larger blocks without exceeding a specified token count. It returns the combined text blocks, their original indices, and the count of chunks dropped due to overflow.\ndef combine_chunks_with_no_minimum(\n        chunks: List[str],\n        max_tokens: int,\n        chunk_delimiter=\"\\n\\n\",\n        header: Optional[str] = None,\n        add_ellipsis_for_overflow=False,\n) -> Tuple[List[str], List[int]]:\n    dropped_chunk_count = 0\n    output = []  # list to hold the final combined chunks\n    output_indices = []  # list to hold the indices of the final combined chunks\n    candidate = (\n        [] if header is None else [header]\n    )  # list to hold the current combined chunk candidate\n    candidate_indices = []\n    for chunk_i, chunk in enumerate(chunks):\n        chunk_with_header = [chunk] if header is None else [header, chunk]\n        if len(tokenize(chunk_delimiter.join(chunk_with_header))) > max_tokens:\n            print(f\"warning: chunk overflow\")\n            if (\n                    add_ellipsis_for_overflow\n                    and len(tokenize(chunk_delimiter.join(candidate + [\"...\"]))) <= max_tokens\n            ):\n                candidate.append(\"...\")\n                dropped_chunk_count += 1\n            continue  # this case would break downstream assumptions\n        # estimate token count with the current chunk added\n        extended_candidate_token_count = len(tokenize(chunk_delimiter.join(candidate + [chunk])))\n        # If the token count exceeds max_tokens, add the current candidate to output and start a new candidate\n        if extended_candidate_token_count > max_tokens:\n            output.append(chunk_delimiter.join(candidate))\n            output_indices.append(candidate_indices)\n            candidate = chunk_with_header  # re-initialize candidate\n            candidate_indices = [chunk_i]\n        # otherwise keep extending the candidate\n        else:\n            candidate.append(chunk)\n            candidate_indices.append(chunk_i)\n    # add the remaining candidate to output if it's not empty\n    if (header is not None and len(candidate) > 1) or (header is None and len(candidate) > 0):\n        output.append(chunk_delimiter.join(candidate))\n        output_indices.append(candidate_indices)\n    return output, output_indices, dropped_chunk_count","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:47:31.267310Z","iopub.execute_input":"2024-07-18T15:47:31.267792Z","iopub.status.idle":"2024-07-18T15:47:31.283131Z","shell.execute_reply.started":"2024-07-18T15:47:31.267758Z","shell.execute_reply":"2024-07-18T15:47:31.282074Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def summarize(text: str,\n              detail: float = 0,\n              model: str = 'gpt-4-turbo',\n              additional_instructions: Optional[str] = None,\n              minimum_chunk_size: Optional[int] = 500,\n              chunk_delimiter: str = \".\",\n              summarize_recursively=False,\n              verbose=False):\n    \"\"\"\n    Summarizes a given text by splitting it into chunks, each of which is summarized individually. \n    The level of detail in the summary can be adjusted, and the process can optionally be made recursive.\n\n    Parameters:\n    - text (str): The text to be summarized.\n    - detail (float, optional): A value between 0 and 1 indicating the desired level of detail in the summary.\n      0 leads to a higher level summary, and 1 results in a more detailed summary. Defaults to 0.\n    - model (str, optional): The model to use for generating summaries. Defaults to 'phamtungthuy/Vistral-7B-Chat-AWQ'.\n    - additional_instructions (Optional[str], optional): Additional instructions to provide to the model for customizing summaries.\n    - minimum_chunk_size (Optional[int], optional): The minimum size for text chunks. Defaults to 500.\n    - chunk_delimiter (str, optional): The delimiter used to split the text into chunks. Defaults to \".\".\n    - summarize_recursively (bool, optional): If True, summaries are generated recursively, using previous summaries for context.\n    - verbose (bool, optional): If True, prints detailed information about the chunking process.\n\n    Returns:\n    - str: The final compiled summary of the text.\n\n    The function first determines the number of chunks by interpolating between a minimum and a maximum chunk count based on the `detail` parameter. \n    It then splits the text into chunks and summarizes each chunk. If `summarize_recursively` is True, each summary is based on the previous summaries, \n    adding more context to the summarization process. The function returns a compiled summary of all chunks.\n    \"\"\"\n\n    # check detail is set correctly\n    assert 0 <= detail <= 1\n\n    # interpolate the number of chunks based to get specified level of detail\n    max_chunks = len(chunk_on_delimiter(text, minimum_chunk_size, chunk_delimiter))\n    min_chunks = 1\n    num_chunks = int(min_chunks + detail * (max_chunks - min_chunks))\n\n    # adjust chunk_size based on interpolated number of chunks\n    document_length = len(tokenize(text))\n    chunk_size = max(minimum_chunk_size, document_length // num_chunks)\n    text_chunks = chunk_on_delimiter(text, chunk_size, chunk_delimiter)\n    if verbose:\n        print(f\"Splitting the text into {len(text_chunks)} chunks to be summarized.\")\n        print(f\"Chunk lengths are {[len(tokenize(x)) for x in text_chunks]}\")\n\n    # set system message\n    system_message_content = \"Rewrite this text in summarized form.\"\n    if additional_instructions is not None:\n        system_message_content += f\"\\n\\n{additional_instructions}\"\n\n    accumulated_summaries = []\n    for chunk in tqdm(text_chunks):\n        if summarize_recursively and accumulated_summaries:\n            # Creating a structured prompt for recursive summarization\n            accumulated_summaries_string = '\\n\\n'.join(accumulated_summaries)\n            user_message_content = f\"Previous summaries:\\n\\n{accumulated_summaries_string}\\n\\nText to summarize next:\\n\\n{chunk}\"\n        else:\n            # Directly passing the chunk for summarization without recursive context\n            user_message_content = chunk\n\n        # Constructing messages based on whether recursive summarization is applied\n        messages = [\n            {\"role\": \"system\", \"content\": system_message_content},\n            {\"role\": \"user\", \"content\": user_message_content}\n        ]\n\n        # Assuming this function gets the completion and works as expected\n        response = get_chat_completion(messages)\n        accumulated_summaries.append(response)\n\n    # Compile final summary from partial summaries\n    final_summary = '\\n\\n'.join(accumulated_summaries)\n\n    return final_summary","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:47:31.284642Z","iopub.execute_input":"2024-07-18T15:47:31.284974Z","iopub.status.idle":"2024-07-18T15:47:31.299275Z","shell.execute_reply.started":"2024-07-18T15:47:31.284951Z","shell.execute_reply":"2024-07-18T15:47:31.298353Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"summary_with_detail_1 = summarize(reference, detail=1, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:49:49.088701Z","iopub.execute_input":"2024-07-18T15:49:49.089070Z","iopub.status.idle":"2024-07-18T15:50:09.072853Z","shell.execute_reply.started":"2024-07-18T15:49:49.089041Z","shell.execute_reply":"2024-07-18T15:50:09.071846Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Splitting the text into 16 chunks to be summarized.\nChunk lengths are [485, 501, 491, 418, 482, 495, 486, 492, 493, 480, 498, 499, 480, 501, 490, 404]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16/16 [00:19<00:00,  1.23s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"score1 = sentence_bleu(conference.split(), summary_with_detail_1.split())\nprint(score1)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:50:11.347457Z","iopub.execute_input":"2024-07-18T15:50:11.348095Z","iopub.status.idle":"2024-07-18T15:50:13.134280Z","shell.execute_reply.started":"2024-07-18T15:50:11.348065Z","shell.execute_reply":"2024-07-18T15:50:13.133229Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"0.19448257948621436\n","output_type":"stream"}]},{"cell_type":"code","source":"calculateSemanticSimilarity(conference,summary_with_detail_1)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T15:50:14.657693Z","iopub.execute_input":"2024-07-18T15:50:14.658420Z","iopub.status.idle":"2024-07-18T15:50:20.160296Z","shell.execute_reply.started":"2024-07-18T15:50:14.658390Z","shell.execute_reply":"2024-07-18T15:50:20.159237Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"0.77080715"},"metadata":{}}]}]}